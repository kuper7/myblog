### 一、基于稀疏先验模型的去混响

内容来自：

《Multi-Channel Linear Prediction-Based Speech Dereverberation With Sparse Priors》

《Generalization of multi-channel linear prediction methods for blind MIMO impulse response shortening》

《Group sparsity forMIMO speech dereverberation》

#### 1摘要：有别于传统的基于概率模型的去混响，文章提出了用稀疏先验建模语音信号，实现去混响。

#### 2去混响方法比较

##### 2.1基于概率模型的去混响

基于概率模型的去混响，将去混响问题转换为下面的优化问题：
$$
{\mathop {\min }\limits_{\lambda > 0,{\bf g}} \sum\limits_{n = 1}^N \left({{{\vert d(n{{)\vert}^2}} \over {\lambda (n)}} + \log \pi \lambda (n)} \right).}{\tag{1}}
$$
上式目标函数包含两组参数：$g,\lambda$。为了估计g，有两步：

###### 2.1.1step1：假设方差是固定值，计算转换为：

$$
{{\hat {\bf g}}^{(i + 1)}} = \mathop{\arg\min}\limits_{{\bf g}}\sum\limits_{n = 1}^N {{\vert d(n{{)\vert}^2}} \over {{\hat{\lambda }^{(i)}}(n)}} = \mathop{\arg\min}\limits_{{\bf g}}\,{{\bf d}^H}{\cal D}_{{\hat{\lambda }^{(i)}}}^{- 1}{\bf d},{\tag{2}}
$$
其中${{\cal D}_{{\hat{\lambda }^{(i)}}}} = diag({\hat{\lambda }^{(i)}})$是构建对角矩阵的函数，除主对角线外其他元素全为0,d是纯净信号，x为观察信号。
$$
{{\hat {\bf {g}}^{(i + 1)}} = {{\left({{\bf X}_\tau ^H{\cal D}_{{\hat{\lambda }^{(i)}}}^{- 1}{{\bf X}_\tau}} \right)}^{- 1}}{\bf X}_\tau ^H{\cal D}_{{\hat{\lambda }^{(i)}}}^{- 1}{{\bf x}_1}.}{\tag{3}}
$$
###### 2.1.2step2:固定g，求解$\lambda$，先计算

$$
{\hat{\bf {d}}(k) = {{\bf x}_1}(k) - {{\bf X}_\tau }(k){\hat {\bf g}}(k),}{\tag{4}}
$$
在计算：
$$
{\hat{\lambda }^{(i + 1)}}(n) = \mathop{\arg\min}\limits_{{\lambda (n) > 0}}{{\vert{\hat{d}^{(i + 1)}}(n{{)\vert}^2}} \over {\lambda (n)}} + \log \pi \lambda (n).{\tag{5}}
$$
结果为：
$$
{\hat{\lambda}^{(i + 1)}}(n) = \vert{\hat{d}^{(i + 1)}}(n{)\vert^2}\tag{6}
$$
简单的写为：
$$
{{\hat{\lambda }^{(i + 1)}} = \vert{{\hat {\bf d}}^{(i + 1)}}{\vert^2},}{\tag{7}}
$$
##### 2.2基于稀疏先验模型的去混响

作者认为利用稀疏先验可以很好地建模语音信号的短时傅立叶变换系数，虽然为了简化计算，语音信号STFT的复数系数的实部和虚部通常被假设为独立的，但是我们已经观察到复数值语音系数的分布实际上是近似**圆高斯分布**

###### 2.2.1稀疏先验的凸表示

直观上，超高斯分布被认为是稀疏的，即在原点处比对应的高斯先验有更高的峰值和更长更重的尾巴。超高斯分布的定义与峰度有关，峰度公式：
$$
Kurt=\frac{\mu^4}{\sigma^4}-3
$$
峰度为0为正态分布/高斯分布，大于0为超高斯分布，小于0为亚高斯分布。

这里我们考虑一个复值随机变量z的一般圆高斯分布：
$$
\rho (z) = {e^{- f(\vert z\vert)}}.{\tag{8}}
$$
下式可以表示一个合适的稀疏先验的概率魔都函数：
$$
\rho (z) = \mathop {\max }\limits_{\lambda > 0} {{\cal N}_{\C}}(z;0,\lambda)\psi (\lambda),{\tag{9}}
$$
其中$\psi(\lambda )$是一个缩放函数。

###### 2.2.2使用广义稀疏先验进行语音去混响

作者提出使用圆稀疏先验密度函数$\rho (d(n)) = {e^{- f(\vert d(n)\vert)}}$来对期望语音信号的STFT系数建模，凸表示为：
$$
\rho (d(n)) = \mathop {\max }\limits_{\lambda (n) > 0} {{\cal N}_{\C}}(d(n);0,\lambda (n))\psi (\lambda (n)).{\tag{10}}
$$
这样的话，我们求g的问题就转变为了，使稀疏度最大：
$$
\mathop {\max }\limits_{\bf g} \prod\limits_{n = 1}^N \mathop {\max }\limits_{\lambda (n) > 0} {{\cal N}_{\C}}(d(n);0,\lambda (n))\psi (\lambda (n)).\tag{11}
$$
这相当于最小化关于预测向量g和方差$\lambda$拟合的对数似然值，即：
$$
{\mathop {\min }\limits_{\lambda > 0,{\bf g}} \sum\limits_{n = 1}^N \left({{{\vert d(n{{)\vert}^2}} \over {\lambda (n)}} + \log \pi \lambda (n) - \log \psi (\lambda (n))} \right),}{\tag{11}}
$$

###### 2.2.2.1估计g

假设方差是固定的，得到与常规方法相同的LS问题，其解由式(3)所示。
$$
{{\hat {\bf {g}}^{(i + 1)}} = {{\left({{\bf X}_\tau ^H{\cal D}_{{\hat{\lambda }^{(i)}}}^{- 1}{{\bf X}_\tau}} \right)}^{- 1}}{\bf X}_\tau ^H{\cal D}_{{\hat{\lambda }^{(i)}}}^{- 1}{{\bf x}_1}.}{\tag{3}}
$$

###### 2.2.2.2估计$\lambda$

假设预测向量固定为$g^{(i+1)}$，通过求解以下问题得到方差
$$
{\hat{\lambda }^{(i + 1)}}(n) = \mathop{\arg\min}\limits_{{\lambda (n) > 0}}{{\vert{\hat{d}^{(i + 1)}}(n{{)\vert}^2}} \over {\lambda (n)}} + \log \pi \lambda (n) - \log \psi (\lambda (n)).{\tag{12}}
$$
对于(8)中的一般稀疏先验分布，其解为：
$$
{\hat{\lambda }^{(i + 1)}}(n) = {{2\vert{\hat{d}^{(i + 1)}}(n)\vert} \over {{f^\prime}\left({\vert{\hat{d}^{(i + 1)}}(n)\vert} \right)}},{\tag{13}}
$$
###### 2.2.3复广义高斯先验

作为零均值圆超高斯先验的一个例子，在本文的其余部分，我们将考虑给定的复广义高斯先验：
$$
\rho (z) = {p \over {2\pi \gamma \Gamma (2/p)}}{e^{- {{\vert z{\vert^p}} \over {{\gamma ^{p/2}}}}}},{\tag{14}}
$$
尺度参数$\gamma > 0$,形状参数$0 < p \leq 2$，$\Gamma(.)$表示伽玛函数。设置p=2得到圆高斯分布，形状参数值越小，先验越稀疏，即零处峰值越大，尾部越重。其密度函数为：
$$
f(t) = {{{t^p}} \over {{\gamma ^{p/2}}}} - \log {p \over {2\pi \gamma \Gamma (2/p)}},{\tag{15}}
$$


<center><img src="https://pangcong1117.github.io/myblog/images/去混响图7.png"/>

对于期望信号符合CGG（复广义高斯先验）时，迭代(i+1)时的最离线方差$\lambda(n)$可用(13)和(15)表示为：
$$
{\hat{\lambda }^{(i + 1)}}(n) = {{2{\gamma ^{p/2}}} \over p}\vert{d^{(i + 1)}}(n{)\vert^{2 - p}}.{\tag{16}}
$$
该表达式取决于式(14)中CGG先验的形状和尺度参数。然而，由于使用(3)估计的g，以及因此使用(4)估计的期望语音信号d，这式子是方差缩放的不变量，因此(16)的更新可以简化为
$$
{{\hat{\lambda }^{(i + 1)}}(n) = \vert{\hat{d}^{(i + 1)}}(n{{)\vert}^{2 - p}},}{\tag{19}}
$$
上式只依赖于CGG先验的形状参数p∈(0,2)。在实践中，为了防止被零除法，一个小的正常数被用于估计方差的下界，即
$$
{\hat{\lambda }^{(i + 1)}} = \max \{\vert{{\hat {\bf d}}^{(i + 1)}}{\vert^{2 - p}},{\varepsilon _{{\min}}}\},{\tag{20}}
$$
我们将这种方法称为WPE-CGG，在算法1中进行了总结:

算法1：

参数有滤波器长度$L_g$，预测延时$\tau$，形状参数p，正则化参数${\varepsilon _{{\min}}}$，最大迭代次数$i_{max}$，容忍上限$\eta$

<center><img src="https://pangcong1117.github.io/myblog/images/去混响图8.png"/>
###### 2.2.4与传统方法的关系

需要注意的是，传统方法中的方差更新(7)对应于建议的更新(19)中的设置p=0。将式(1)中的优化问题与式(11)中的优化问题进行比较，可以看出，传统的方法是将比例(缩放)函数$\psi(.)$设为定值。因此，对于传统的方法，期望信号的先验，正如在(11)中的缩放函数设为1，等于:
$$
\rho (d(n)) = \mathop {\max }\limits_{\lambda (n) > 0} {{{e^{- {{\vert d(n{{)\vert}^2}} \over {\lambda (n)}}}}} \over {\pi \lambda (n)}} = {{{e^{- 1}}} \over {\pi \vert d(n{{)\vert}^2}}} \propto {1 \over {\vert d(n{{)\vert}^2}}}{\tag{21}}
$$
在$\lambda(n)=|d(n)|$时达到最大值2。得到的先验也可以用式(8)表示为:
$$
f(t) = \log {t^2} + {\rm const}.{\tag{22}}
$$
#### 3.ℓp-Norm最小化

作者对传统的WPE方法和本文提出的WPE-CGG方法估算预测向量g，依据的ℓp-norm最小化问题进行表述,旨在更好地理解所提出方法背后的成本函数和相关问题的稀疏恢复。对于一般先验参数p(.)和独立系数d(n)，似然函数为
$$
{\cal L}\left({\bf g} \right) = \prod\limits_{n = 1}^N \rho \left({d(n)} \right).{\tag{23}}
$$
因此，对于式(8)中的稀疏先验p(.)，可以通过最小化负对数似然来得到预测向量g的ML(极大似然)估计，即。
$$
{\hat {\bf g}} = \mathop{\arg\min}\limits_{{\bf g}}\sum\limits_{n = 1}^N f\left({\vert d(n)\vert} \right).{\tag{24}}
$$
对于与(14)中一样的CGG先验ρ(.)，使用(15)可以得到这个ML估计值，作为以下问题的解决方案
$$
{\mathop {\min }\limits_{\bf g} \Vert {\bf d}\Vert_p^p,}{\tag{25}}
$$
其中$\Vert.{\Vert _p}$是ℓp-Norm被定义为$\Vert {\bf d}{\Vert _p} = {({\sum\nolimits_{n = 1}^N \vert d(n{{)\vert}^p}})^{1/p}}$

对于(21)中给出的先验概率p(.)的传统方法，利用(22)得到预测向量的ML估计，为
$$
\mathop {\min }\limits_{\bf g} \sum\limits_{n = 1}^N \log \vert d(n)\vert.{\tag{26}}
$$


这个对数成本函数通常用于信号处理问题如得到ℓ0-norm的近似即度量向量中非零元素的个数。ℓ0-norm与前面定义的ℓp-norm关系为$\Vert {\bf d}{\Vert _0} = \mathop {\lim }\nolimits_{p \rightarrow 0} \sum\nolimits_{n = 1}^N \vert d(n{)\vert^p}$。对数惩罚函数可以通过ℓ0-norm表示为：$\mathop {\lim }\nolimits_{p \rightarrow 0} {1 \over p}\sum\nolimits_{n = 1}^N ({\vert d(n{{)\vert}^p} - 1}) = \sum\nolimits_{n = 1}^N \log \vert d(n)\vert$。此外，式(26)中优化问题的局部极小集对应于优化问题的局部极小集:
$$
\mathop {\min }\limits_{\bf g} \Vert {\bf d}{\Vert _0}.{\tag{27}}
$$


### 二、数值优化问题（Numerical Optimization）

求解带约束的最优化问题，一类很重要的方法就是将约束添加到目标函数中，从而转换为一系列子问题进行求解，最终逼近最优解。关键问题是如何将约束进行转换。

#### 1.二次惩罚方法

带约束问题如果转换为目标函数加上一个对约束的惩罚项，则问题转换为一个无约束问题。转换后的问题可以通过惩罚项的系数进行控制，一个比较常见的惩罚函数就是二次惩罚。

等式约束最优化问题：

例如：求解
$$
min \ f(x)  s.t \ c_i(x)=0,\; i \in \mathcal E
$$


添加一个二次惩罚项，则有
$$
Q(x;\mu)=f(x) + \frac \mu 2\sum_{i \in \mathcal E}c_i^2(x)
$$
其中 μ是惩罚参数，直观上只要增加惩罚参数的值就可以逼近原始问题的最优解。在实际中，对于某个惩罚参数 μ只要几步无约束最优化就可以了，不需要寻找最优解。

一般化约束最优化问题：

例如：
$$
min f(x) \\ s.t \; c_i(x)=0 \ i \in \mathcal E \\ \ \ \ \ \ c_i(x) \ge 0 \ i \in \mathcal I
$$
添加惩罚项系数结果为:
$$
Q(x;\mu)=f(x) + \frac \mu 2\sum_{i \in \mathcal E}c_i^2(x)+\frac \mu 2\sum_{i \in \mathcal I}([c_i(x)]^-)^2
$$
其中$c_i(x)^-$表示当该值大于0时，结果为0，否则为$−c_i(x)$

#### 2.二次惩罚项通用框架:

<center><img src="https://pangcong1117.github.io/myblog/images/去混响图9.png"/>



非平滑惩罚函数：

有些惩罚函数是精确的，即惩罚项参数μ达到一定值时转换后的问题的最优解就是原始问题的最优解，其中$l1$惩罚就是精确的，表示如下：
$$
\phi_1(x;\mu)=f(x)+\mu \sum_{i \in \mathcal E}|c_i(x)|+\mu \sum_{i \in \mathcal E}|c_i(x)|^-
$$

<center><img src="https://pangcong1117.github.io/myblog/images/去混响图10.png"/>

#### 3.增广拉格朗日方法

增广拉格朗日方法在拉格朗日方法的基础上添加了二次惩罚项，从而使得转换后的问题能够更容易求解，不至于因条件数变大不好求。则转换后的问题为
$$
\mathcal L(x,\lambda;\mu)=f(x)-\sum_{i \in \mathcal E}\lambda_i c_i(x)+\frac \mu 2 \sum_{i \in \mathcal E}c_i(x)^2
$$
在第K步迭代过程中，固定惩罚项参数μ和λk，此时优化x，根据最优化条件有:
$$
\nabla_x\mathcal L=\nabla f(x)-\sum_{i \in \mathcal E}(\lambda^k_i-\mu_kc_i(x))\nabla c_i(x)=0
$$
对比最优性条件，应该有$\nabla f(x^*)=0;\lambda^*=\lambda^k_i-\mu_kc_i(x)$，从而很自然的得$\lambda^{k+1}=\lambda^k_i-\mu_kc_i(x)$.

<center><img src="https://pangcong1117.github.io/myblog/images/去混响图11.png"/>

在实际中，增广拉格朗日方法可以很有效的处理边界约束和线性约束最优化问题。



### 三、《Adaptive Speech Dereverberation Using ConstrainedSparse Multichannel Linear Prediction》去混响方法

#### 1.大概内容：

没有使用传统的概率模型引出自适应解，而是通过语音稀疏特效的方法。文章在《Multi-Channel Linear Prediction-Based Speech Dereverberation With Sparse Priors》的基础上，为了防止遗忘因子过小时对语音过度预测的动态场景，使用增广拉格朗日的优化算法。

#### 2.分析

基于稀疏度的去混响问题自适应解如下：
$$
\begin{equation}
\hat{ {\boldsymbol {\mathrm{G}}} }(n) = \hat{ {\boldsymbol {\mathrm{Q}}} }^{-1}(n) \hat{ {\boldsymbol {\mathrm{R}}}
 }(n).
\end{equation}
$$

$$
\begin{align}
\hat{ {\boldsymbol {\mathrm{Q}}} }(n) =& \sum _{t=1}^n \gamma ^{n-t} w(t) \tilde{ {\boldsymbol {\mathrm{x}}} }_\tau
 (t) \tilde{ {\boldsymbol {\mathrm{x}}} }_\tau ^ {\mathsf {H}} (t), \nonumber\\
\hat{ {\boldsymbol {\mathrm{R}}} }(n)=& \sum _{t=1}^n \gamma ^{n-t} w(t) \tilde{ {\boldsymbol {\mathrm{x}}} }_\tau
 (t) {\boldsymbol {\mathrm{x}}} ^ {\mathsf {H}} (t)
\end{align}
$$

观察发现，就是基于概率模型的MCLP的自适应解，在离线解的基础下加了个指数因子，以实现快速收敛。

去混响问题转换为了最优化问题：
$$
\begin{align}
\min _{ {\boldsymbol {\mathrm{G}}} (n), {\boldsymbol {\mathrm{z}}} (n)} &F \left({\boldsymbol {\mathrm{G}}} (n)
 \right) + C \left({\boldsymbol {\mathrm{z}}} (n) \right) \nonumber \\
&\qquad \quad \ \ \text{subject to} \quad {\boldsymbol {\mathrm{G}}} ^ {\mathsf {H}} (n) \tilde{ {\boldsymbol
 {\mathrm{x}}} }_\tau (n) = {\boldsymbol {\mathrm{z}}} (n)
\end{align}
$$
为了防止遗忘因子过小时对语音过度预测，使用增广拉格朗日方法，即在拉格朗日方法的基础上添加了二次惩罚项，从而使得转换后的问题能够更容易求解。转换后的问题变为了
$$
\begin{align}
&\mathcal {L} \left({\boldsymbol {\mathrm{G}}} (n), {\boldsymbol {\mathrm{z}}} (n), {\boldsymbol {\mathrm{\lambda
 }}} (n) \right) = F \left({\boldsymbol {\mathrm{G}}} (n) \right) + C \left({\boldsymbol {\mathrm{z}}} (n) \right)
 \nonumber \\
&+ \frac{\rho }{2} \Vert {\boldsymbol {\mathrm{G}}} ^ {\mathsf {H}} (n) \tilde{ {\boldsymbol {\mathrm{x}}} }_\tau
 (n) - {\boldsymbol {\mathrm{z}}} (n) - {\boldsymbol {\mathrm{\lambda }}} (n) \Vert _2^2 - \frac{\rho }{2} \Vert
 {\boldsymbol {\mathrm{\lambda }}} (n) \Vert _2^2
\end{align}
$$
其中，p参数是惩罚参数。ADMM算法通过交替地对G(n)和z(n)最小化L来进行,在第i此迭代中：
$$
\begin{align}
\check{ {\boldsymbol {\mathrm{G}}} }^{i}(n) = \mathcal {S}_{\rho, \tilde{ {\boldsymbol {\mathrm{x}}} }_\tau (n)}^F
 \left(\left(\check{ {\boldsymbol {\mathrm{z}}} }^{i-1}(n) + {\boldsymbol {\mathrm{\lambda }}} ^{i-1}(n) \right)^
 {\mathsf {H}} \right)\\
\check{ {\boldsymbol {\mathrm{z}}} }^{i}(n) = \mathcal {S}_{\rho, {\boldsymbol {\mathrm{I}}} }^C \left(\left(\check{
 {\boldsymbol {\mathrm{G}}} }^{i}(n) \right)^H \tilde{ {\boldsymbol {\mathrm{x}}} }_\tau (n) - {\boldsymbol
 {\mathrm{\lambda }}} ^{i-1}(n) \right)\\
{\boldsymbol {\mathrm{\lambda }}} ^{i}(n) = {\boldsymbol {\mathrm{\lambda }}} ^{i-1}(n) + \check{ {\boldsymbol
 {\mathrm{z}}} }^{i}(n) - \left(\check{ {\boldsymbol {\mathrm{G}}} }^{i}(n) \right)^H \tilde{ {\boldsymbol
 {\mathrm{x}}} }_\tau (n)
\end{align}
$$
部分过程还未理解，未完待续